# High Latency Fault Injection Configuration
# This configuration tests the observability stack's ability to handle slow responses
# Simulates degraded API performance with high latency for both LLM and tool calls

# Mode: mock (fault injection works with mock providers)
mode: mock

# Agent configuration
agent:
  agent_id: canary_fault_latency_001
  agent_name: High Latency Fault Canary

# Mock provider base settings
mock_settings:
  # Base latency (will be overridden by fault injection)
  llm_latency_ms: 500
  tool_latency_ms: 200
  tool_failure_rate: 0.0

# Fault injection configuration
fault_injection:
  # Enable fault injection
  enabled: true
  
  # Active fault profiles
  profiles:
    - high_latency
  
  # High latency profile configuration
  high_latency:
    # LLM response latency: 8 seconds (simulates slow API)
    llm_latency_ms: 8000
    
    # Tool execution latency: 5 seconds (simulates slow external services)
    tool_latency_ms: 5000

# OpenTelemetry configuration
otlp:
  # OTLP collector endpoint
  endpoint: http://localhost:4317
  insecure: true

# Scenarios to execute
# All standard scenarios run with high latency to test timeout handling
scenarios:
  - simple_tool_call      # Tests basic latency handling
  - multi_tool_chain      # Tests cumulative latency across multiple calls
  - tool_failure          # Tests latency + error handling combination
  - high_token_usage      # Tests latency with large payloads
  - conversation_context  # Tests latency in multi-turn conversations
  - multi_agent          # Tests latency propagation in agent hierarchies

# Telemetry validation
# Validates that high latency is correctly captured in metrics and spans
validation:
  prometheus_url: http://localhost:9090
  opensearch_url: https://localhost:9200
  opensearch_user: admin
  opensearch_password: ${OPENSEARCH_PASSWORD}
